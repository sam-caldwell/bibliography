<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>A Practical Quantum Instruction Set Architecture</title>
  <style>
    body{font:16px/1.5 system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,sans-serif;max-width:860px;margin:2rem auto;padding:0 1rem;color:#222}
    header,footer{color:#666}
    h1{font-size:1.6rem;margin:0.2rem 0 0.6rem}
    .meta{margin:0.4rem 0}
    .block{margin:0.8rem 0}
    a{color:#0b66d6;text-decoration:none}
    a:hover{text-decoration:underline}
    .kw span{display:inline-block;background:#eef;padding:2px 6px;margin:2px;border-radius:10px;font-size:0.85em}
  </style>
  <link rel="canonical" href="article/3ba7de32-814e-4634-af3c-e317cee4500a.html" />
  <meta name="description" content="Annotated citation for A Practical Quantum Instruction Set Architecture" />
  <meta property="og:title" content="A Practical Quantum Instruction Set Architecture" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="article/3ba7de32-814e-4634-af3c-e317cee4500a.html" />
  <meta property="og:see_also" content="https://doi.org/10.48550/arxiv.1608.03355" />
  <meta name="citation_doi" content="10.48550/ARXIV.1608.03355" />
  <meta name="citation_title" content="A Practical Quantum Instruction Set Architecture" />
  <meta name="citation_author" content="Smith, R. S." />
  <meta name="citation_author" content="Curtis, M. J." />
  <meta name="citation_author" content="Zeng, W. J." />
  
  
  
  
  
</head>
<body>
  <header><a href="../index.html">‚Üê Index</a></header>
  <main>
    <h1>A Practical Quantum Instruction Set Architecture</h1>
    <div class="meta">
      <div><strong>Type:</strong> article</div>
      
      <div><strong>Authors:</strong> Smith, R. S., Curtis, M. J., Zeng, W. J.</div>
      
      
      
      <div><strong>Publisher:</strong> arXiv</div>
      
      <div><strong>DOI:</strong> <a href="https://doi.org/10.48550/ARXIV.1608.03355">10.48550/ARXIV.1608.03355</a></div>
      <div><strong>URL:</strong> <a href="https://doi.org/10.48550/arxiv.1608.03355">https://doi.org/10.48550/arxiv.1608.03355</a></div>
      <div><strong>ID:</strong> 3ba7de32-814e-4634-af3c-e317cee4500a</div>
    </div>
    <div class="block">
      <h2>Summary</h2>
      <p>The work titled &#34;Attention Is All You Need&#34; introduces a novel neural network architecture known as the Transformer,
which is designed for sequence-to-sequence tasks in natural language processing. The authors, Vaswani et al., propose a
model that relies entirely on self-attention mechanisms, eschewing the recurrent and convolutional layers commonly used
in previous architectures. This approach allows for greater parallelization during training and improves the model&#39;s
ability to capture long-range dependencies in data. The Transformer architecture consists of an encoder and a decoder,
each composed of multiple layers that utilize self-attention and feed-forward neural networks. The authors detail the
multi-head attention mechanism, which enables the model to focus on different parts of the input sequence
simultaneously, enhancing its contextual understanding. Additionally, they introduce positional encoding to retain the
sequential information of the input data. The paper presents extensive experiments on translation tasks, demonstrating
that the Transformer outperforms existing models in terms of both accuracy and training efficiency. The authors conclude
that the self-attention mechanism is a powerful alternative to traditional methods, paving the way for future research
and applications in various domains of machine learning and natural language processing.</p>
    </div>
    
    <div class="block kw">
      <h3>Keywords</h3>
      <span>quantum mechanics</span><span>quantum computing</span><span>computer science</span><span>programming</span><span>instruction set architecture</span><span>isa</span>
    </div>
    
  </main>
  <footer><small>Generated by bib publish</small></footer>
</body>
</html>
